---
permalink: /
title: "Welcome to Shuyong Gao's homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currectly a Postdoctoral Fellow (Shanghai Super Postdoctoral Fellow) at <a href="https://www.fudanroilab.com/"> Lab of Robotics Oriented Intelligence </a>, School of Computer Science, Fudan University, China, mentored by Prof. <a href="https://faet.fudan.edu.cn/e4/28/c23898a255016/page.htm"> Wenqiang Zhang </a>. I received my Ph.D. degree from School of Computer Science, Fudan University, China, in Jan. 2023, advised by Prof. <a href="https://faet.fudan.edu.cn/e4/28/c23898a255016/page.htm"> Wenqiang Zhang </a>.


<!--

--> 

Preprints
----
(✉️ indicates corresponding author, and † indicates equal contribution)
<table style="width:100%; border-collapse: collapse; border: none;">
  <tr>
    <th width="40%" style="border: none;">
      <img src="../paper_images/msvcod.png" width="350"/>
    </th>
    <th style="text-align:left; border: none;" width="60%">
            <span style="font-size:18px">MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection </span><br>
            <span style="font-weight:normal;font-size:16px"> <b>Shuyong Gao</b>, Yuang Feng, Qishan Wang, Lingyi Hong, Xinyu Zhou, Liu Fei, Yan Wang, Wenqiang Zhang<sup>✉️</sup></span><br>
            <span style="font-weight:border:;font-size:16px"> <em>arXiv preprint, 2025 (CVPR 2026 under review)</em> </span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="">Paper</a>][<a>Code</a>][<a href="https://shuyonggao.github.io/MSVCOD"> Project Page </a>]</span>
    </th>
  </tr> 
</table>


<table style="width:100%; border-collapse: collapse; border: none;">
  <tr>
    <th width="40%" style="border: none;">
      <img src="../paper_images/P3S-Diffusion.png" width="350"/>
    </th>
    <th style="text-align:left; border: none;" width="60%">
            <span style="font-size:18px">P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision </span><br>
            <span style="font-weight:normal;font-size:16px">Junjie Hu, <b>Shuyong Gao<sup>✉️</sup></b>, Lingyi Hong, Qishan Wang, Yuzhou Zhao, Yan Wang, Wenqiang Zhang<sup>✉️</sup></span><br>
            <span style="font-weight:border:;font-size:16px"> <em>arXiv preprint, 2024 (TNNLS under review)</em> </span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2412.19533">Paper</a>][<a>Code</a>][<a> Project Page </a>]</span>
    </th>
  </tr> 
</table>

<table style="width:100%; border-collapse: collapse; border: none;">
  <tr>
    <th width="40%" style="border: none;">
      <img src="../paper_images/holistically_cod.png" width="350"/>
    </th>
    <th style="text-align:left; border: none;" width="60%">
            <span style="font-size:18px">A Holistically Point-guided Text Framework for Weakly-Supervised Camouflaged Object Detection </span><br>
            <span style="font-weight:normal;font-size:16px">Tsui Qin Mok<sup>†</sup>, <b>Shuyong Gao<sup>✉️†</sup></b>, Haozhe Xing, Miaoyang He, Yan Wang, Wenqiang Zhang<sup>✉️</sup></span><br>
            <span style="font-weight:border;font-size:16px"> <em> arXiv preprint, 2025 (IJCV under review) </em> </span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2501.06038">Paper</a>][<a href="https://github.com/shuyonggao/HPGT_WSCOD"> Code</a>][<a> Project Page </a>]</span>
    </th>
  </tr> 
</table>


<table style="width:100%; border-collapse: collapse; border: none;">
  <tr>
    <th width="40%" style="border: none;">
      <img src="../paper_images/boosting_sod.png" width="350"/>
    </th>
    <th style="text-align:left; border: none;" width="60%">
            <span style="font-size:18px">Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models </span><br>
            <span style="font-weight:normal;font-size:16px">Miaoyang He, <b>Shuyong Gao<sup>✉️</sup></b>, Tsui Qin Mok, Weifeng Ge, Wenqiang Zhang<sup>✉️</sup></span><br>
            <span style="font-weight:border;font-size:16px"> <em> arXiv preprint, 2025 (TMM under review)</em> </span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2501.04582">Paper</a>][<a> Code</a>][<a> Project Page </a>]</span>
    </th>
  </tr> 
</table>

Selected Publications
------

<table style="width:100%; border-collapse: collapse; border: none;">
  <tr>
    <th width="40%" style="border: none;">
      <img src="../paper_images/top_down_sod.png" width="350"/>
    </th>
    <th style="text-align:left; border: none;" width="60%">
            <span style="font-size:18px">Towards End-to-End Unsupervised Saliency Detection with Self-Supervised Top-Down Context </span><br>
            <span style="font-weight:normal;font-size:16px"> Yicheng Song, <b>Shuyong Gao<sup>✉️</sup></b>, Haozhe Xing, and Yiting Cheng, Yan Wang, Wenqiang Zhang<sup>✉️</sup></span><br>
            <span style="font-weight:border:;font-size:16px"> <em>ACM International Conference on Multimedia (ACM MM), 2023</em> </span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2310.09533">Paper</a>][<a href="https://github.com/rejoicesyc/STC"> Code </a>]</span>
    </th>
  </tr> 
</table>




Research Interests
------

News and Olds
------

